{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b79c547",
   "metadata": {},
   "source": [
    "The system has two LSTM layers with 50 units, <br>\n",
    "one for each context side, which concatenates the <br>\n",
    "outputs and passes that to a feedforward layer <br>\n",
    "with 64 neurons, followed by a dropout layer at <br>\n",
    "rate 0.5, and a final one-neuron output layer of <br>\n",
    "sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17209607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the things\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699993d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data and vocab\n",
    "def load_data(file_name):\n",
    "    data = []\n",
    "    vocab = {\"<UNK>\":0}\n",
    "    with open(\"train.jsonl\",'r') as file:\n",
    "            for line in file.readlines():\n",
    "                    line = json.loads(line)\n",
    "                    #create vocabulary from all unique words in all sentences\n",
    "                    line[\"sentence1\"] = line[\"sentence1\"].strip('.').strip(',').strip(\"?\").strip(\"!\").strip(\";\").strip(\":\")\n",
    "                    line[\"sentence2\"] = line[\"sentence2\"].strip('.').strip(',').strip(\"?\").strip(\"!\").strip(\";\").strip(\":\")\n",
    "                    sentence = line['sentence1'] + \" \" + line['sentence2']\n",
    "                    #strip all punctuation from vocab words\n",
    "                    words = sentence.split()\n",
    "                    #add if not already in vocab\n",
    "                    for word in words:\n",
    "                        if word not in vocab:\n",
    "                            #add word to vocab dict\n",
    "                            vocab[word] = len(vocab)\n",
    "                    #add line to data\n",
    "                    data.append(line)\n",
    "    return vocab, data\n",
    "\n",
    "def sen2vec(s):\n",
    "    v = [vocab[word] for word in s.split()]\n",
    "    return tensor(v).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d30b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim): # output = number tags\n",
    "        super().__init__()\n",
    "        \n",
    "        # if option 1 change embedding_dim in LSTM to 2*embedding_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, 1, bias=False)\n",
    "        self.lstm2 = nn.LSTM(embedding_dim, hidden_dim, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.output_layer = nn.Linear(2*hidden_dim, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "\n",
    "        embed1 = torch.sum(self.embedding(s1),dim=1)\n",
    "        _,(hidden_rep1,_) = self.lstm1(embed1.unsqueeze(0))\n",
    "\n",
    "        embed2 = torch.sum(self.embedding(s2),dim=1)\n",
    "        _,(hidden_rep2,_) = self.lstm2(embed2.unsqueeze(0))\n",
    "\n",
    "        # Option 1 Concat\n",
    "        hidden_rep1 = hidden_rep1.squeeze(0).squeeze(0)\n",
    "        hidden_rep2 = hidden_rep2.squeeze(0).squeeze(0)\n",
    "\n",
    "        final_hidden_rep = torch.cat((hidden_rep1, hidden_rep2))\n",
    "\n",
    "        drop = self.dropout(final_hidden_rep)\n",
    "\n",
    "        output = self.sigmoid(self.output_layer(drop.squeeze(0)))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec31a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, train_data = load_data('train.jsonl')\n",
    "_, test_data = load_data('test.jsonl')\n",
    "_, val_data = load_data('val.jsonl')\n",
    "\n",
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b8ec727",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_wic = NeuralNet(len(vocab),50,64,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a89b30b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "# Model Train \n",
    "\n",
    "epochs = 10\n",
    "ce = nn.CrossEntropyLoss()\n",
    "softmax = nn.Softmax(dim=0)\n",
    "optimizer = optim.SGD(our_wic.parameters(), lr=0.1)\n",
    "\n",
    "our_wic.train()\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(\"Epoch:\",i)\n",
    "    for point in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # a) calculate probs / get an output\n",
    "        s1 = sen2vec(point[\"sentence1\"])\n",
    "        s2 = sen2vec(point[\"sentence2\"])\n",
    "        y_raw = our_wic(s1,s2)\n",
    "        #y_hat = softmax(y_raw)\n",
    "        \n",
    "        y = tensor(int(point[\"label\"]))\n",
    "        # b) compute loss\n",
    "        loss = ce(y_raw.unsqueeze(0),y.unsqueeze(0))\n",
    "        \n",
    "        # c) get the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # d) update the weights\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "468cef97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9174649963154016\n"
     ]
    }
   ],
   "source": [
    "our_wic.eval()\n",
    "\n",
    "score = 0\n",
    "for point in val_data:\n",
    "    s1 = sen2vec(point['sentence1'])\n",
    "    s2 = sen2vec(point['sentence2'])\n",
    "    output = our_wic(s1,s2)\n",
    "    result = torch.argmax(softmax(output))\n",
    "    if bool(result) == point[\"label\"]:\n",
    "        score += 1\n",
    "\n",
    "print(score/len(val_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
