{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports all the libraries we need\n",
    "import torch\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_transformers import *\n",
    "import numpy as np\n",
    "import json\n",
    "import collections\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes a sentence input and returns a tensor vector\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_data basically reads in data --> takes in everything from jsonl files\n",
    "def load_data(filename):\n",
    "  data = []\n",
    "  # read in each line and add it to list\n",
    "  with open(filename, mode = \"r\") as file:\n",
    "      for line in file:\n",
    "          data.append(json.loads(line))\n",
    "  return data\n",
    "\n",
    "#Takes a list of words (strings) and a sentence (as a RoBERTa tokenized ID list) and returns a list\n",
    "#of pairs indicating the tokens' start and end positions in the sentence for each word\n",
    "      \n",
    "def find_word_in_tokenized_sentence(word,token_ids): #gets the same word in a sentence\n",
    "  decomposedWord = tokenizer.encode(word)\n",
    "  # Iterate through to find a matching sublist of the token_ids\n",
    "  for i in range(len(token_ids)):\n",
    "    if token_ids[i] == decomposedWord[0] and token_ids[i:i+len(decomposedWord)] == decomposedWord:\n",
    "      return (i,i+len(decomposedWord)-1) #matching word found\n",
    "  # if no matching word --> return this \n",
    "  return (-1,-1)\n",
    "  \n",
    "def find_words_in_tokenized_sentences(wordList,token_ids): #gets list of word positions in a sentence\n",
    "  intList = []\n",
    "  for word in wordList:\n",
    "    if len(intList) == 0:\n",
    "      intList.append(find_word_in_tokenized_sentence(word,token_ids))\n",
    "    else:\n",
    "      afterLastInterval = intList[-1][1]+1\n",
    "      interv = find_word_in_tokenized_sentence(word,token_ids[afterLastInterval:])\n",
    "      actualPositions = (interv[0] + afterLastInterval,interv[1]+afterLastInterval)\n",
    "      intList.append(actualPositions)\n",
    "  return intList #returns list of positions\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels, return_predict_correctness = False):\n",
    "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "  labels_flat = labels.flatten()\n",
    "  if return_predict_correctness:\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat), pred_flat == labels_flat\n",
    "  else:\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 #decreased the size until the CPU stops dying\n",
    "EPOCHS = 10 #could do more for higher accuracy buts takes too long\n",
    "PATIENCE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing function\n",
    "def wic_preprocessing(json_objects, training = True, shuffle_data = False):\n",
    "  wic_sentences = [] #create arrays for sentences, encoded, etc. \n",
    "  wic_encoded = []\n",
    "  wic_labels = []\n",
    "  wic_word_locs = []\n",
    "  wic_indexes = []\n",
    "  for index, example in enumerate(json_objects):\n",
    "    wic_indexes.append(index)\n",
    "    #combine the sentences with marks at beginnings and ends and add to array\n",
    "    sentence = f\"<s>{example['sentence1']}</s><s>{example['sentence2']}</s>\"\n",
    "    wic_sentences.append(sentence)\n",
    "    #encode sentences\n",
    "    wic_encoded.append(tokenizer.encode(sentence, add_special_tokens=False))\n",
    "    #keep track of matching word\n",
    "    word = example['word']\n",
    "    word_locs = (-1, -1)\n",
    "    # Split the sentences\n",
    "    sent1_split = example['sentence1'].split(' ')\n",
    "    sent2_split = example['sentence2'].split(' ')\n",
    "    # keep track of word indices in sentences\n",
    "    sent1_word_char_loc = (example['start1'], example['end1'])\n",
    "    sent2_word_char_loc = (example['start2'], example['end2'])\n",
    "    # # of characters parsed in each sentence\n",
    "    sent_chars = 0\n",
    "    # Loop through first sentence\n",
    "    i, j = 0, 0\n",
    "    word1_not_found, word2_not_found = True, True\n",
    "    \n",
    "\n",
    "    while word1_not_found and i < len(sent1_split):\n",
    "      word_len = len(sent1_split[i])\n",
    "      # Found index of word in sentence\n",
    "      if sent_chars >= sent1_word_char_loc[0] or sent_chars + word_len >= sent1_word_char_loc[1]:\n",
    "        word_locs = (i, -1) \n",
    "        word1_not_found = False\n",
    "      #haven't found word yet, so use prev word\n",
    "      elif sent_chars > sent1_word_char_loc[1]:\n",
    "        word_locs = (i - 1, -1)\n",
    "        word1_not_found = False\n",
    "      else:\n",
    "        # Look at the next word\n",
    "        sent_chars += word_len + 1 # Plus one for the space\n",
    "        i += 1\n",
    "    # Loop over the words in the second\n",
    "    sent_chars = 0\n",
    "    while word2_not_found and j < len(sent2_split):\n",
    "      word_len = len(sent2_split[j])\n",
    "      if sent_chars >= sent2_word_char_loc[0] or sent_chars + word_len >= sent2_word_char_loc[1]:\n",
    "        word_locs = (i, j) \n",
    "        word2_not_found = False\n",
    "      elif sent_chars > sent2_word_char_loc[1]:\n",
    "        word_locs = (i, j - 1)\n",
    "        word2_not_found = False\n",
    "      else:\n",
    "        # Look at the next word\n",
    "        sent_chars += word_len + 1 # Plus one for the space\n",
    "        j += 1\n",
    "    # split out punctuation and find word index for tokenized sentences\n",
    "    word1 = sent1_split[word_locs[0]].translate(str.maketrans('', '', string.punctuation)) \n",
    "    word2 = sent2_split[word_locs[1]].translate(str.maketrans('', '', string.punctuation))\n",
    "    token_word_locs = find_words_in_tokenized_sentences([word1, word2], wic_encoded[-1])\n",
    "    wic_word_locs.append(token_word_locs)\n",
    "    # get label if there is one avail\n",
    "    if training:\n",
    "      if example['label']:\n",
    "        wic_labels.append(1)\n",
    "      else:\n",
    "        wic_labels.append(0)\n",
    "  # Pad the sequences and find the encoded word location in the combined input\n",
    "  max_len = np.array([len(ex) for ex in wic_encoded]).max()\n",
    "  wic_padded = {\"input_ids\" : [], \"attention_mask\" : [], \"token_type_ids\" : [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : wic_indexes}\n",
    "  for i in range(0, len(wic_encoded)):\n",
    "    enc_sentence = wic_encoded[i]\n",
    "    word_locs = wic_word_locs[i]\n",
    "    # Pad sequences\n",
    "    ex_len = len(enc_sentence)\n",
    "    padded_sentence = enc_sentence.copy()\n",
    "    padded_sentence.extend([0]*(max_len - ex_len))\n",
    "    wic_padded[\"input_ids\"].append(padded_sentence)\n",
    "    padded_mask = [1] * ex_len\n",
    "    padded_mask.extend([0]*(max_len - ex_len))\n",
    "    wic_padded[\"attention_mask\"].append(padded_mask)\n",
    "    # Create the vector to get back the words after RoBERTa\n",
    "    token_word_locs = wic_word_locs[i]\n",
    "    first_word_loc = []\n",
    "    second_word_loc = []\n",
    "    len_first_word = token_word_locs[0][1] - token_word_locs[0][0] + 1\n",
    "    len_second_word = token_word_locs[1][1] - token_word_locs[1][0] + 1\n",
    "    for j in range(0, max_len):\n",
    "      if j >= token_word_locs[0][0] and j <= token_word_locs[0][1]:\n",
    "        #Part of the first word\n",
    "        first_word_loc.append(1.0 / len_first_word)\n",
    "      else:\n",
    "        first_word_loc.append(0.0)\n",
    "      if j >= token_word_locs[1][0] and j <= token_word_locs[1][1]:\n",
    "        #Part of the second word\n",
    "        second_word_loc.append(1.0 / len_second_word)\n",
    "      else:\n",
    "        second_word_loc.append(0.0)\n",
    "    # encapsulate max_len in list\n",
    "    wic_padded[\"word1_locs\"].append([first_word_loc])\n",
    "    wic_padded[\"word2_locs\"].append([second_word_loc])\n",
    "    # token_type_ids is a mask that tells where the first and second sentences are\n",
    "    token_type_id = []\n",
    "    first_sentence = True\n",
    "    sentence_start = True\n",
    "    for token in padded_sentence:\n",
    "      if first_sentence and sentence_start and token == 0:\n",
    "        # Allows 0 at the start of the first sentence\n",
    "        token_type_id.append(0)\n",
    "      elif first_sentence and token > 0:\n",
    "        if sentence_start:\n",
    "          sentence_start = False\n",
    "        token_type_id.append(0)\n",
    "      elif first_sentence and not sentence_start and token == 0:\n",
    "        first_sentence = False\n",
    "        # Start of second sentence\n",
    "        token_type_id.append(1)\n",
    "      else:\n",
    "        # Second sentence\n",
    "        token_type_id.append(1)\n",
    "    wic_padded[\"token_type_ids\"].append(token_type_id)\n",
    "  if training:\n",
    "    if shuffle_data:\n",
    "      # Shuffle the data\n",
    "      raw_set = {\"input_ids\": [], \"token_type_ids\": [], \"attention_mask\": [], \"labels\": [], \"word1_locs\": [], \"word2_locs\" : [], \"index\" : []}\n",
    "      raw_set[\"input_ids\"], raw_set[\"token_type_ids\"], raw_set[\"attention_mask\"], raw_set[\"labels\"], raw_set[\"word1_locs\"], raw_set[\"word2_locs\"], raw_set[\"index\"] = shuffle(\n",
    "          wic_padded[\"input_ids\"], wic_padded[\"token_type_ids\"], wic_padded[\"attention_mask\"], wic_labels, wic_padded[\"word1_locs\"], wic_padded[\"word2_locs\"], wic_padded[\"index\"])\n",
    "    else:\n",
    "      raw_set = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"],\n",
    "                 \"attention_mask\": wic_padded[\"attention_mask\"], \"labels\": wic_labels, \"index\" : wic_padded[\"index\"],\n",
    "                 \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "  else: # No labels present (Testing set)\n",
    "    # Do not shuffle the testing set\n",
    "    raw_set = {\"input_ids\": wic_padded[\"input_ids\"], \"token_type_ids\": wic_padded[\"token_type_ids\"], \n",
    "               \"attention_mask\": wic_padded[\"attention_mask\"], \"index\" : wic_padded[\"index\"], \n",
    "               \"word1_locs\": wic_padded[\"word1_locs\"], \"word2_locs\" : wic_padded[\"word2_locs\"]}\n",
    "  # Return the raw data (Need to put them in a PyTorch tensor and dataset)\n",
    "  return raw_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'ball', 'sentence1': 'The ball was already emptying out before the fire alarm sounded.', 'sentence2': 'The ball rolled into the corner pocket.', 'idx': 939, 'label': False, 'start1': 4, 'start2': 4, 'end1': 8, 'end2': 8, 'version': 1.1}\n",
      "[0, 20, 1011, 21, 416, 38470, 4048, 66, 137, 5, 668, 8054, 12020, 4, 2, 0, 20, 1011, 6387, 88, 5, 2797, 7524, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0\n",
      "[[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n"
     ]
    }
   ],
   "source": [
    "# Process the data\n",
    "train_json_objs = load_data(\"train.jsonl\")\n",
    "raw_train_set = wic_preprocessing(train_json_objs, shuffle_data=True)\n",
    "print(train_json_objs[raw_train_set[\"index\"][15]])\n",
    "print(raw_train_set[\"input_ids\"][15]),\n",
    "print(raw_train_set[\"token_type_ids\"][15]),\n",
    "print(raw_train_set[\"attention_mask\"][15]),\n",
    "print(raw_train_set[\"labels\"][15])\n",
    "print(raw_train_set[\"word1_locs\"][15])\n",
    "print(raw_train_set[\"word2_locs\"][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.625\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_train_set[\"labels\"])/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch dataset for it\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(raw_train_set[\"input_ids\"]),\n",
    "    torch.tensor(raw_train_set[\"token_type_ids\"]),\n",
    "    torch.tensor(raw_train_set[\"attention_mask\"]),\n",
    "    torch.tensor(raw_train_set[\"labels\"]),\n",
    "    torch.tensor(raw_train_set[\"word1_locs\"]),\n",
    "    torch.tensor(raw_train_set[\"word2_locs\"]),\n",
    "    torch.tensor(raw_train_set[\"index\"])\n",
    ")\n",
    "# Create a sampler and loader\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the val json objects\n",
    "valid_json_objs = load_data(\"val.jsonl\")\n",
    "# preprocess the objects\n",
    "raw_valid_set = wic_preprocessing(valid_json_objs)\n",
    "# Create PyTorch dataset\n",
    "validation_data = TensorDataset(\n",
    "    torch.tensor(raw_valid_set[\"input_ids\"]),\n",
    "    torch.tensor(raw_valid_set[\"token_type_ids\"]),\n",
    "    torch.tensor(raw_valid_set[\"attention_mask\"]),\n",
    "    torch.tensor(raw_valid_set[\"labels\"]),\n",
    "    torch.tensor(raw_valid_set[\"word1_locs\"]),\n",
    "    torch.tensor(raw_valid_set[\"word2_locs\"]),\n",
    "    torch.tensor(raw_valid_set[\"index\"])\n",
    ")\n",
    "# Create a sampler and loader for each\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base RoBERTa model\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "roberta_init_weights = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WiC_Head(torch.nn.Module):\n",
    "    def __init__(self, roberta_based_model, embedding_size = 768):\n",
    "        \"\"\"\n",
    "        Keeps a reference to the provided RoBERTa model. \n",
    "        It then adds a linear layer that takes the distance between two \n",
    "        \"\"\"\n",
    "        super(WiC_Head, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedder = roberta_based_model\n",
    "        self.linear_diff = torch.nn.Linear(embedding_size, 250, bias = True)\n",
    "        self.linear_seperator = torch.nn.Linear(250, 2, bias = True)\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, word1_locs = None, word2_locs = None):\n",
    "        \"\"\"\n",
    "        Takes in the same argument as RoBERTa forward plus two tensors for the location of the 2 words to compare\n",
    "        \"\"\"\n",
    "        batch_size = word1_locs.shape[0]\n",
    "        # Get the embeddings\n",
    "        embs, _ = self.embedder.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Get the words\n",
    "        word1s = torch.matmul(word1_locs, embs).view(batch_size, self.embedding_size)\n",
    "        word2s = torch.matmul(word2_locs, embs).view(batch_size, self.embedding_size)\n",
    "        diff = word1s - word2s\n",
    "        # Calculate outputs using activation\n",
    "        layer1_results = self.activation(self.linear_diff(diff))\n",
    "        logits = self.softmax(self.linear_seperator(layer1_results))\n",
    "        outputs = logits\n",
    "        # Calculate the loss\n",
    "        if labels is not None:\n",
    "            loss = self.loss(logits.view(-1, 2), labels.view(-1))\n",
    "            outputs = (loss, logits)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = WiC_Head(model, embedding_size = 768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_716/4179931766.py:35: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  logits = self.softmax(self.linear_seperator(layer1_results))\n",
      "/home/arinard/.local/lib/python3.8/site-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tTraining Batch 10: Loss: 0.6926242113113403; Accuracy: 0.53125\n",
      "\t\tTraining Batch 20: Loss: 0.6947644352912903; Accuracy: 0.5\n",
      "\t\tTraining Batch 30: Loss: 0.6976891756057739; Accuracy: 0.59375\n",
      "\t\tTraining Batch 40: Loss: 0.6930834054946899; Accuracy: 0.59375\n",
      "\t\tTraining Batch 50: Loss: 0.6902619004249573; Accuracy: 0.4375\n",
      "\t\tTraining Batch 60: Loss: 0.6937728524208069; Accuracy: 0.46875\n",
      "\t\tTraining Batch 70: Loss: 0.6829476356506348; Accuracy: 0.6875\n",
      "\t\tTraining Batch 80: Loss: 0.6894336342811584; Accuracy: 0.53125\n",
      "\t\tTraining Batch 90: Loss: 0.6788446307182312; Accuracy: 0.59375\n",
      "\t\tTraining Batch 100: Loss: 0.6814977526664734; Accuracy: 0.65625\n",
      "\t\tTraining Batch 110: Loss: 0.6697793006896973; Accuracy: 0.59375\n",
      "\t\tTraining Batch 120: Loss: 0.6848355531692505; Accuracy: 0.5625\n",
      "\t\tTraining Batch 130: Loss: 0.6859109401702881; Accuracy: 0.5625\n",
      "\t\tTraining Batch 140: Loss: 0.6424288749694824; Accuracy: 0.59375\n",
      "\t\tTraining Batch 150: Loss: 0.652489423751831; Accuracy: 0.65625\n",
      "\t\tTraining Batch 160: Loss: 0.6818639636039734; Accuracy: 0.65625\n",
      "\t\tTraining Batch 170: Loss: 0.6583741903305054; Accuracy: 0.7\n",
      "Training:\n",
      "\tLoss: 0.682835761238547; Accuracy: 0.5759926470588236\n",
      "\t\tValidation Batch 10: Loss: 0.6864540576934814; Accuracy: 0.59375\n",
      "\t\tValidation Batch 20: Loss: 0.6484234929084778; Accuracy: 0.6666666666666666\n",
      "Validation:\n",
      "\tLoss=0.6558272778987885; Accuracy: 0.6395833333333333\n",
      "Training epoch #2\n",
      "\t\tTraining Batch 10: Loss: 0.6293536424636841; Accuracy: 0.71875\n",
      "\t\tTraining Batch 20: Loss: 0.6245498061180115; Accuracy: 0.71875\n",
      "\t\tTraining Batch 30: Loss: 0.555311381816864; Accuracy: 0.8125\n",
      "\t\tTraining Batch 40: Loss: 0.5611016154289246; Accuracy: 0.71875\n",
      "\t\tTraining Batch 50: Loss: 0.6259533762931824; Accuracy: 0.625\n",
      "\t\tTraining Batch 60: Loss: 0.6355135440826416; Accuracy: 0.65625\n",
      "\t\tTraining Batch 70: Loss: 0.5704407095909119; Accuracy: 0.71875\n",
      "\t\tTraining Batch 80: Loss: 0.5138117074966431; Accuracy: 0.8125\n",
      "\t\tTraining Batch 90: Loss: 0.6474760174751282; Accuracy: 0.65625\n",
      "\t\tTraining Batch 100: Loss: 0.6868872046470642; Accuracy: 0.5625\n",
      "\t\tTraining Batch 110: Loss: 0.626397967338562; Accuracy: 0.625\n",
      "\t\tTraining Batch 120: Loss: 0.6532260179519653; Accuracy: 0.625\n",
      "\t\tTraining Batch 130: Loss: 0.605566680431366; Accuracy: 0.6875\n",
      "\t\tTraining Batch 140: Loss: 0.5293963551521301; Accuracy: 0.75\n",
      "\t\tTraining Batch 150: Loss: 0.537912130355835; Accuracy: 0.8125\n",
      "\t\tTraining Batch 160: Loss: 0.5678942799568176; Accuracy: 0.65625\n",
      "\t\tTraining Batch 170: Loss: 0.6604610085487366; Accuracy: 0.6\n",
      "Training:\n",
      "\tLoss: 0.5869600622092976; Accuracy: 0.7140073529411765\n",
      "\t\tValidation Batch 10: Loss: 0.6708052158355713; Accuracy: 0.59375\n",
      "\t\tValidation Batch 20: Loss: 0.5780337452888489; Accuracy: 0.6666666666666666\n",
      "Validation:\n",
      "\tLoss=0.6169706881046295; Accuracy: 0.6677083333333333\n",
      "Training epoch #3\n",
      "\t\tTraining Batch 10: Loss: 0.48933956027030945; Accuracy: 0.8125\n",
      "\t\tTraining Batch 20: Loss: 0.6340858340263367; Accuracy: 0.625\n",
      "\t\tTraining Batch 30: Loss: 0.49700263142585754; Accuracy: 0.78125\n",
      "\t\tTraining Batch 40: Loss: 0.5284261703491211; Accuracy: 0.78125\n",
      "\t\tTraining Batch 50: Loss: 0.43466317653656006; Accuracy: 0.875\n",
      "\t\tTraining Batch 60: Loss: 0.5502064824104309; Accuracy: 0.75\n",
      "\t\tTraining Batch 70: Loss: 0.4362398684024811; Accuracy: 0.875\n",
      "\t\tTraining Batch 80: Loss: 0.5224699974060059; Accuracy: 0.78125\n",
      "\t\tTraining Batch 90: Loss: 0.5351207256317139; Accuracy: 0.78125\n",
      "\t\tTraining Batch 100: Loss: 0.46854403614997864; Accuracy: 0.84375\n",
      "\t\tTraining Batch 110: Loss: 0.3939957022666931; Accuracy: 0.9375\n",
      "\t\tTraining Batch 120: Loss: 0.504775881767273; Accuracy: 0.78125\n",
      "\t\tTraining Batch 130: Loss: 0.46996888518333435; Accuracy: 0.84375\n",
      "\t\tTraining Batch 140: Loss: 0.6035955548286438; Accuracy: 0.6875\n",
      "\t\tTraining Batch 150: Loss: 0.5197815299034119; Accuracy: 0.8125\n",
      "\t\tTraining Batch 160: Loss: 0.6124740242958069; Accuracy: 0.6875\n",
      "\t\tTraining Batch 170: Loss: 0.5030743479728699; Accuracy: 0.85\n",
      "Training:\n",
      "\tLoss: 0.5032183589304194; Accuracy: 0.8070220588235294\n",
      "\t\tValidation Batch 10: Loss: 0.6445943117141724; Accuracy: 0.65625\n",
      "\t\tValidation Batch 20: Loss: 0.587393581867218; Accuracy: 0.7\n",
      "Validation:\n",
      "\tLoss=0.6283851772546768; Accuracy: 0.6646875\n",
      "Training epoch #4\n",
      "\t\tTraining Batch 10: Loss: 0.4308338463306427; Accuracy: 0.90625\n",
      "\t\tTraining Batch 20: Loss: 0.4715040326118469; Accuracy: 0.875\n",
      "\t\tTraining Batch 30: Loss: 0.4813520312309265; Accuracy: 0.78125\n",
      "\t\tTraining Batch 40: Loss: 0.46560460329055786; Accuracy: 0.84375\n",
      "\t\tTraining Batch 50: Loss: 0.5086470246315002; Accuracy: 0.8125\n",
      "\t\tTraining Batch 60: Loss: 0.45270904898643494; Accuracy: 0.875\n",
      "\t\tTraining Batch 70: Loss: 0.4936572015285492; Accuracy: 0.78125\n",
      "\t\tTraining Batch 80: Loss: 0.41752389073371887; Accuracy: 0.875\n",
      "\t\tTraining Batch 90: Loss: 0.45479390025138855; Accuracy: 0.84375\n",
      "\t\tTraining Batch 100: Loss: 0.4611488878726959; Accuracy: 0.84375\n",
      "\t\tTraining Batch 110: Loss: 0.44047755002975464; Accuracy: 0.875\n",
      "\t\tTraining Batch 120: Loss: 0.515265941619873; Accuracy: 0.75\n",
      "\t\tTraining Batch 130: Loss: 0.4504407048225403; Accuracy: 0.84375\n",
      "\t\tTraining Batch 140: Loss: 0.40383270382881165; Accuracy: 0.9375\n",
      "\t\tTraining Batch 150: Loss: 0.4684136211872101; Accuracy: 0.84375\n",
      "\t\tTraining Batch 160: Loss: 0.466943621635437; Accuracy: 0.84375\n",
      "\t\tTraining Batch 170: Loss: 0.5171874761581421; Accuracy: 0.85\n",
      "Training:\n",
      "\tLoss: 0.45859781941946814; Accuracy: 0.8511397058823529\n",
      "\t\tValidation Batch 10: Loss: 0.631334125995636; Accuracy: 0.6875\n",
      "\t\tValidation Batch 20: Loss: 0.6151795983314514; Accuracy: 0.7\n",
      "Validation:\n",
      "\tLoss=0.6104817479848862; Accuracy: 0.6896875\n",
      "Training epoch #5\n",
      "\t\tTraining Batch 10: Loss: 0.4865726828575134; Accuracy: 0.8125\n",
      "\t\tTraining Batch 20: Loss: 0.400564968585968; Accuracy: 0.9375\n",
      "\t\tTraining Batch 30: Loss: 0.4895443618297577; Accuracy: 0.84375\n",
      "\t\tTraining Batch 40: Loss: 0.539897620677948; Accuracy: 0.75\n",
      "\t\tTraining Batch 50: Loss: 0.4109525680541992; Accuracy: 0.90625\n",
      "\t\tTraining Batch 60: Loss: 0.43508878350257874; Accuracy: 0.875\n",
      "\t\tTraining Batch 70: Loss: 0.4358963370323181; Accuracy: 0.875\n",
      "\t\tTraining Batch 80: Loss: 0.34459903836250305; Accuracy: 0.96875\n",
      "\t\tTraining Batch 90: Loss: 0.39167365431785583; Accuracy: 0.9375\n",
      "\t\tTraining Batch 100: Loss: 0.3515297770500183; Accuracy: 0.96875\n",
      "\t\tTraining Batch 110: Loss: 0.4537457823753357; Accuracy: 0.875\n",
      "\t\tTraining Batch 120: Loss: 0.4293777048587799; Accuracy: 0.90625\n",
      "\t\tTraining Batch 130: Loss: 0.36649441719055176; Accuracy: 0.9375\n",
      "\t\tTraining Batch 140: Loss: 0.40516719222068787; Accuracy: 0.9375\n",
      "\t\tTraining Batch 150: Loss: 0.3924872875213623; Accuracy: 0.9375\n",
      "\t\tTraining Batch 160: Loss: 0.4812260568141937; Accuracy: 0.84375\n",
      "\t\tTraining Batch 170: Loss: 0.3454605042934418; Accuracy: 1.0\n",
      "Training:\n",
      "\tLoss: 0.4357787740581176; Accuracy: 0.8772058823529412\n",
      "\t\tValidation Batch 10: Loss: 0.6327561140060425; Accuracy: 0.65625\n",
      "\t\tValidation Batch 20: Loss: 0.5755358338356018; Accuracy: 0.7333333333333333\n",
      "Validation:\n",
      "\tLoss=0.60425665974617; Accuracy: 0.7007291666666666\n",
      "Training epoch #6\n",
      "\t\tTraining Batch 10: Loss: 0.36131641268730164; Accuracy: 0.96875\n",
      "\t\tTraining Batch 20: Loss: 0.4377105236053467; Accuracy: 0.90625\n",
      "\t\tTraining Batch 30: Loss: 0.38472414016723633; Accuracy: 0.96875\n",
      "\t\tTraining Batch 40: Loss: 0.3897625803947449; Accuracy: 0.9375\n",
      "\t\tTraining Batch 50: Loss: 0.46950435638427734; Accuracy: 0.84375\n",
      "\t\tTraining Batch 60: Loss: 0.3869151473045349; Accuracy: 0.9375\n",
      "\t\tTraining Batch 70: Loss: 0.34790873527526855; Accuracy: 0.96875\n",
      "\t\tTraining Batch 80: Loss: 0.42358145117759705; Accuracy: 0.90625\n",
      "\t\tTraining Batch 90: Loss: 0.4410838484764099; Accuracy: 0.84375\n",
      "\t\tTraining Batch 100: Loss: 0.35870522260665894; Accuracy: 0.9375\n",
      "\t\tTraining Batch 110: Loss: 0.32063812017440796; Accuracy: 1.0\n",
      "\t\tTraining Batch 120: Loss: 0.4136753976345062; Accuracy: 0.90625\n",
      "\t\tTraining Batch 130: Loss: 0.43787601590156555; Accuracy: 0.875\n",
      "\t\tTraining Batch 140: Loss: 0.42292508482933044; Accuracy: 0.90625\n",
      "\t\tTraining Batch 150: Loss: 0.3618454039096832; Accuracy: 0.9375\n",
      "\t\tTraining Batch 160: Loss: 0.47937723994255066; Accuracy: 0.8125\n",
      "\t\tTraining Batch 170: Loss: 0.5004306435585022; Accuracy: 0.8\n",
      "Training:\n",
      "\tLoss: 0.4091917080037734; Accuracy: 0.904889705882353\n",
      "\t\tValidation Batch 10: Loss: 0.5438607931137085; Accuracy: 0.75\n",
      "\t\tValidation Batch 20: Loss: 0.5892972350120544; Accuracy: 0.7333333333333333\n",
      "Validation:\n",
      "\tLoss=0.5930217355489731; Accuracy: 0.7085416666666666\n",
      "Training epoch #7\n",
      "\t\tTraining Batch 10: Loss: 0.34379902482032776; Accuracy: 0.96875\n",
      "\t\tTraining Batch 20: Loss: 0.34309452772140503; Accuracy: 0.96875\n",
      "\t\tTraining Batch 30: Loss: 0.343974769115448; Accuracy: 0.96875\n",
      "\t\tTraining Batch 40: Loss: 0.3603435456752777; Accuracy: 0.96875\n",
      "\t\tTraining Batch 50: Loss: 0.42036640644073486; Accuracy: 0.90625\n",
      "\t\tTraining Batch 60: Loss: 0.3368052840232849; Accuracy: 0.96875\n",
      "\t\tTraining Batch 70: Loss: 0.40060490369796753; Accuracy: 0.90625\n",
      "\t\tTraining Batch 80: Loss: 0.3657231330871582; Accuracy: 0.96875\n",
      "\t\tTraining Batch 90: Loss: 0.3639012575149536; Accuracy: 0.9375\n",
      "\t\tTraining Batch 100: Loss: 0.4013735055923462; Accuracy: 0.90625\n",
      "\t\tTraining Batch 110: Loss: 0.35754016041755676; Accuracy: 0.9375\n",
      "\t\tTraining Batch 120: Loss: 0.34909915924072266; Accuracy: 0.96875\n",
      "\t\tTraining Batch 130: Loss: 0.40758147835731506; Accuracy: 0.90625\n",
      "\t\tTraining Batch 140: Loss: 0.4101024568080902; Accuracy: 0.90625\n",
      "\t\tTraining Batch 150: Loss: 0.4200119376182556; Accuracy: 0.90625\n",
      "\t\tTraining Batch 160: Loss: 0.3661916255950928; Accuracy: 0.9375\n",
      "\t\tTraining Batch 170: Loss: 0.4116167426109314; Accuracy: 0.9\n",
      "Training:\n",
      "\tLoss: 0.3988721884348813; Accuracy: 0.9124632352941177\n",
      "\t\tValidation Batch 10: Loss: 0.6559768915176392; Accuracy: 0.65625\n",
      "\t\tValidation Batch 20: Loss: 0.6048034429550171; Accuracy: 0.7\n",
      "Validation:\n",
      "\tLoss=0.6186293691396714; Accuracy: 0.6865625\n",
      "Training epoch #8\n",
      "\t\tTraining Batch 10: Loss: 0.45748862624168396; Accuracy: 0.84375\n",
      "\t\tTraining Batch 20: Loss: 0.36051198840141296; Accuracy: 0.96875\n",
      "\t\tTraining Batch 30: Loss: 0.41906800866127014; Accuracy: 0.90625\n",
      "\t\tTraining Batch 40: Loss: 0.41076532006263733; Accuracy: 0.90625\n",
      "\t\tTraining Batch 50: Loss: 0.40777280926704407; Accuracy: 0.90625\n",
      "\t\tTraining Batch 60: Loss: 0.49165573716163635; Accuracy: 0.8125\n",
      "\t\tTraining Batch 70: Loss: 0.3677598834037781; Accuracy: 0.9375\n",
      "\t\tTraining Batch 80: Loss: 0.41336292028427124; Accuracy: 0.90625\n",
      "\t\tTraining Batch 90: Loss: 0.3170868456363678; Accuracy: 1.0\n",
      "\t\tTraining Batch 100: Loss: 0.3145166337490082; Accuracy: 1.0\n",
      "\t\tTraining Batch 110: Loss: 0.42278122901916504; Accuracy: 0.90625\n",
      "\t\tTraining Batch 120: Loss: 0.39227768778800964; Accuracy: 0.90625\n",
      "\t\tTraining Batch 130: Loss: 0.3718917667865753; Accuracy: 0.9375\n",
      "\t\tTraining Batch 140: Loss: 0.3911723494529724; Accuracy: 0.90625\n",
      "\t\tTraining Batch 150: Loss: 0.3842453062534332; Accuracy: 0.9375\n",
      "\t\tTraining Batch 160: Loss: 0.31586578488349915; Accuracy: 1.0\n",
      "\t\tTraining Batch 170: Loss: 0.3729349374771118; Accuracy: 0.95\n",
      "Training:\n",
      "\tLoss: 0.38352968833025763; Accuracy: 0.9305882352941176\n",
      "\t\tValidation Batch 10: Loss: 0.6374476552009583; Accuracy: 0.6875\n",
      "\t\tValidation Batch 20: Loss: 0.5659205913543701; Accuracy: 0.7666666666666667\n",
      "Validation:\n",
      "\tLoss=0.6108484625816345; Accuracy: 0.6977083333333334\n",
      "Training epoch #9\n",
      "\t\tTraining Batch 10: Loss: 0.47301578521728516; Accuracy: 0.84375\n",
      "\t\tTraining Batch 20: Loss: 0.3570807874202728; Accuracy: 0.9375\n",
      "\t\tTraining Batch 30: Loss: 0.37470996379852295; Accuracy: 0.9375\n",
      "\t\tTraining Batch 40: Loss: 0.3487280011177063; Accuracy: 0.96875\n",
      "\t\tTraining Batch 50: Loss: 0.37562036514282227; Accuracy: 0.9375\n",
      "\t\tTraining Batch 60: Loss: 0.3491165339946747; Accuracy: 0.96875\n",
      "\t\tTraining Batch 70: Loss: 0.34601473808288574; Accuracy: 0.96875\n",
      "\t\tTraining Batch 80: Loss: 0.41753172874450684; Accuracy: 0.875\n",
      "\t\tTraining Batch 90: Loss: 0.3150864243507385; Accuracy: 1.0\n",
      "\t\tTraining Batch 100: Loss: 0.3450562357902527; Accuracy: 0.96875\n",
      "\t\tTraining Batch 110: Loss: 0.4090937674045563; Accuracy: 0.90625\n",
      "\t\tTraining Batch 120: Loss: 0.4113906919956207; Accuracy: 0.90625\n",
      "\t\tTraining Batch 130: Loss: 0.44758740067481995; Accuracy: 0.84375\n",
      "\t\tTraining Batch 140: Loss: 0.35026106238365173; Accuracy: 0.96875\n",
      "\t\tTraining Batch 150: Loss: 0.3719823956489563; Accuracy: 0.9375\n",
      "\t\tTraining Batch 160: Loss: 0.34793809056282043; Accuracy: 0.9375\n",
      "\t\tTraining Batch 170: Loss: 0.36394697427749634; Accuracy: 0.95\n",
      "Training:\n",
      "\tLoss: 0.37336195321644056; Accuracy: 0.9408823529411764\n",
      "\t\tValidation Batch 10: Loss: 0.528214693069458; Accuracy: 0.78125\n",
      "\t\tValidation Batch 20: Loss: 0.6119793057441711; Accuracy: 0.6666666666666666\n",
      "Validation:\n",
      "\tLoss=0.6007582321763039; Accuracy: 0.7005208333333333\n",
      "Training epoch #10\n",
      "\t\tTraining Batch 10: Loss: 0.39775964617729187; Accuracy: 0.90625\n",
      "\t\tTraining Batch 20: Loss: 0.3849071264266968; Accuracy: 0.90625\n",
      "\t\tTraining Batch 30: Loss: 0.3289887607097626; Accuracy: 0.96875\n",
      "\t\tTraining Batch 40: Loss: 0.4334574341773987; Accuracy: 0.90625\n",
      "\t\tTraining Batch 50: Loss: 0.3811967074871063; Accuracy: 0.9375\n",
      "\t\tTraining Batch 60: Loss: 0.37600627541542053; Accuracy: 0.9375\n",
      "\t\tTraining Batch 70: Loss: 0.37272292375564575; Accuracy: 0.9375\n",
      "\t\tTraining Batch 80: Loss: 0.3231833875179291; Accuracy: 1.0\n",
      "\t\tTraining Batch 90: Loss: 0.3432404100894928; Accuracy: 0.96875\n",
      "\t\tTraining Batch 100: Loss: 0.4290318489074707; Accuracy: 0.875\n",
      "\t\tTraining Batch 110: Loss: 0.38502857089042664; Accuracy: 0.9375\n",
      "\t\tTraining Batch 120: Loss: 0.3270740211009979; Accuracy: 1.0\n",
      "\t\tTraining Batch 130: Loss: 0.39389440417289734; Accuracy: 0.90625\n",
      "\t\tTraining Batch 140: Loss: 0.38163310289382935; Accuracy: 0.90625\n",
      "\t\tTraining Batch 150: Loss: 0.400499165058136; Accuracy: 0.90625\n",
      "\t\tTraining Batch 160: Loss: 0.3571911156177521; Accuracy: 0.96875\n",
      "\t\tTraining Batch 170: Loss: 0.3909876048564911; Accuracy: 0.9\n",
      "Training:\n",
      "\tLoss: 0.3729254871606827; Accuracy: 0.9396691176470588\n",
      "\t\tValidation Batch 10: Loss: 0.5722107887268066; Accuracy: 0.71875\n",
      "\t\tValidation Batch 20: Loss: 0.6216274499893188; Accuracy: 0.7\n",
      "Validation:\n",
      "\tLoss=0.6079147264361382; Accuracy: 0.6959375\n",
      "Best accuracy (0.7085416666666666) obtained at epoch #6.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ideal min accuracy\n",
    "MIN_ACCURACY = 0.73\n",
    "REACHED_MIN_ACCURACY = False\n",
    "best_weights = Model.state_dict()\n",
    "# maximize from 0\n",
    "max_val_acc = (0, 0)\n",
    "# Create the optimizer\n",
    "param_optimizer = list(Model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "# I use the one that comes with the models, but any other optimizer could be used\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=1e-5)\n",
    "# Store our loss and accuracy for plotting\n",
    "fit_history = {\"loss\": [],  \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "epoch_number = 0\n",
    "epoch_since_max = 0\n",
    "continue_learning = True\n",
    "while epoch_number < EPOCHS and continue_learning:\n",
    "  epoch_number += 1\n",
    "  print(f\"Training epoch #{epoch_number}\")\n",
    "\n",
    "  # Tracking variables\n",
    "  tr_loss, tr_accuracy = 0, 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "  # Training\n",
    "  Model.train()\n",
    "\n",
    "  Model.embedder.requires_grad_ = False\n",
    "  # Train the data for each epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "    #reset gradient\n",
    "    optimizer.zero_grad()\n",
    "    # get input and compute loss\n",
    "    loss, logits = Model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, word1_locs = b_word1, word2_locs = b_word2) \n",
    "    # get gradient\n",
    "    loss.backward()\n",
    "    # Update model\n",
    "    optimizer.step()\n",
    "    \n",
    "    logits = logits.detach().numpy()\n",
    "    label_ids = b_labels.numpy()\n",
    "    # Calculate the accuracy\n",
    "    b_accuracy = flat_accuracy(logits, label_ids) # For RobertaForClassification\n",
    "    # Append to fit history\n",
    "    fit_history[\"loss\"].append(loss.item()) \n",
    "    fit_history[\"accuracy\"].append(b_accuracy) \n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    tr_accuracy += b_accuracy\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "    if nb_tr_steps%10 == 0:\n",
    "      print(\"\\t\\tTraining Batch {}: Loss: {}; Accuracy: {}\".format(nb_tr_steps, loss.item(), b_accuracy))\n",
    "  print(\"Training:\\n\\tLoss: {}; Accuracy: {}\".format(tr_loss/nb_tr_steps, tr_accuracy/nb_tr_steps))\n",
    "  \n",
    "  # Validation\n",
    "  Model.eval()\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    b_input_ids, b_token_ids, b_input_mask, b_labels, b_word1, b_word2, b_index = batch\n",
    "    # don't store gradients\n",
    "    with torch.no_grad():\n",
    "      # get input and compute loss\n",
    "      loss, logits = Model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, word1_locs = b_word1, word2_locs = b_word2)\n",
    "\n",
    "    logits = logits.detach().numpy()\n",
    "    label_ids = b_labels.numpy()\n",
    "    # Calculate the accuracy\n",
    "    b_accuracy = flat_accuracy(logits, label_ids) # For RobertaForClassification\n",
    "    # Append to fit history\n",
    "    fit_history[\"val_loss\"].append(loss.item()) \n",
    "    fit_history[\"val_accuracy\"].append(b_accuracy) \n",
    "    # Update tracking variables\n",
    "    eval_loss += loss.item()\n",
    "    eval_accuracy += b_accuracy\n",
    "    nb_eval_examples += b_input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "    if nb_eval_steps%10 == 0:\n",
    "      print(\"\\t\\tValidation Batch {}: Loss: {}; Accuracy: {}\".format(nb_eval_steps, loss.item(), b_accuracy))\n",
    "  eval_acc = eval_accuracy/nb_eval_steps\n",
    "  if eval_acc >= max_val_acc[0]:\n",
    "    max_val_acc = (eval_acc, epoch_number)\n",
    "    continue_learning = True\n",
    "    epoch_since_max = 0 # New max\n",
    "    best_weights = copy.deepcopy(Model.state_dict()) # Keep the best weights\n",
    "    # See if we have reached min_accuracy\n",
    "    if eval_acc >= MIN_ACCURACY:\n",
    "      REACHED_MIN_ACCURACY = True\n",
    "    if REACHED_MIN_ACCURACY:\n",
    "      continue_learning = False # Stop learning. Reached baseline acc for this model\n",
    "  else:\n",
    "    epoch_since_max += 1\n",
    "    if epoch_since_max > PATIENCE:\n",
    "      continue_learning = False # Stop learning, starting to overfit\n",
    "  print(\"Validation:\\n\\tLoss={}; Accuracy: {}\".format(eval_loss/nb_eval_steps, eval_accuracy/nb_eval_steps))\n",
    "print(f\"Best accuracy ({max_val_acc[0]}) obtained at epoch #{max_val_acc[1]}.\")\n",
    "# Reload the best weights (from memory)\n",
    "Model.load_state_dict(best_weights)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
