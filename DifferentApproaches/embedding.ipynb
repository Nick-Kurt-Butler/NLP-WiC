{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b79c547",
   "metadata": {},
   "source": [
    "The system has two LSTM layers with 50 units, <br>\n",
    "one for each context side, which concatenates the <br>\n",
    "outputs and passes that to a feedforward layer <br>\n",
    "with 64 neurons, followed by a dropout layer at <br>\n",
    "rate 0.5, and a final one-neuron output layer of <br>\n",
    "sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17209607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5fdc6ff130>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the things\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "699993d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data and vocab\n",
    "def load_data(file_name):\n",
    "    data = []\n",
    "    vocab = {\"<UNK>\":0}\n",
    "    with open(file_name,'r') as file:\n",
    "            for line in file.readlines():\n",
    "                    line = json.loads(line)\n",
    "                    #create vocabulary from all unique words in all sentences\n",
    "                    line[\"sentence1\"] = line[\"sentence1\"].strip('.').strip(',').strip(\"?\").strip(\"!\").strip(\";\").strip(\":\")\n",
    "                    line[\"sentence2\"] = line[\"sentence2\"].strip('.').strip(',').strip(\"?\").strip(\"!\").strip(\";\").strip(\":\")\n",
    "                    sentence = line['sentence1'] + \" \" + line['sentence2']\n",
    "                    #strip all punctuation from vocab words\n",
    "                    words = sentence.split()\n",
    "                    #add if not already in vocab\n",
    "                    for word in words:\n",
    "                        if word not in vocab:\n",
    "                            #add word to vocab dict\n",
    "                            vocab[word] = len(vocab)\n",
    "                    #add line to data\n",
    "                    data.append(line)\n",
    "    return vocab, data\n",
    "\n",
    "def sen2vec(s):\n",
    "    v = []\n",
    "    for word in s.split():\n",
    "        try:\n",
    "            v.append(vocab[word])\n",
    "        except:\n",
    "            v.append(0)\n",
    "    return tensor(v).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d30b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim): # output = number tags\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.output_layer = nn.Linear(2*hidden_dim, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "\n",
    "        embed1 = torch.sum(self.embedding(s1),dim=1)\n",
    "        _,(hidden_rep1,_) = self.lstm(embed1.unsqueeze(0))\n",
    "\n",
    "        embed2 = torch.sum(self.embedding(s2),dim=1)\n",
    "        _,(hidden_rep2,_) = self.lstm(embed2.unsqueeze(0))\n",
    "\n",
    "        # Option 1 Concat\n",
    "        hidden_rep1 = hidden_rep1.squeeze(0).squeeze(0)\n",
    "        hidden_rep2 = hidden_rep2.squeeze(0).squeeze(0)\n",
    "\n",
    "        final_hidden_rep = torch.cat((hidden_rep1, hidden_rep2))\n",
    "\n",
    "        drop = self.dropout(final_hidden_rep)\n",
    "\n",
    "        output = self.sigmoid(self.output_layer(drop.squeeze(0)))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec31a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, train_data = load_data('train.jsonl')\n",
    "_, test_data = load_data('test.jsonl')\n",
    "_, val_data = load_data('val.jsonl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b8ec727",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_wic = NeuralNet(len(vocab),73,127,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b30b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "tensor(0.6889, grad_fn=<DivBackward0>)\n",
      "0.6175386882829772\n",
      "0.5266457680250783\n",
      "Epoch: 1\n",
      "tensor(0.6695, grad_fn=<DivBackward0>)\n",
      "0.6588061901252763\n",
      "0.542319749216301\n",
      "Epoch: 2\n",
      "tensor(0.6509, grad_fn=<DivBackward0>)\n",
      "0.7076271186440678\n",
      "0.5188087774294671\n",
      "Epoch: 3\n",
      "tensor(0.6289, grad_fn=<DivBackward0>)\n",
      "0.7485261606484893\n",
      "0.512539184952978\n",
      "Epoch: 4\n",
      "tensor(0.6052, grad_fn=<DivBackward0>)\n",
      "0.7855563743551953\n",
      "0.5094043887147336\n",
      "Epoch: 5\n",
      "tensor(0.5840, grad_fn=<DivBackward0>)\n",
      "0.817243920412675\n",
      "0.5094043887147336\n",
      "Epoch: 6\n",
      "tensor(0.5635, grad_fn=<DivBackward0>)\n",
      "0.8465364775239499\n",
      "0.5235109717868338\n",
      "Epoch: 7\n",
      "tensor(0.5420, grad_fn=<DivBackward0>)\n",
      "0.868828297715549\n",
      "0.5203761755485894\n",
      "Epoch: 8\n",
      "tensor(0.5272, grad_fn=<DivBackward0>)\n",
      "0.8761974944731025\n",
      "0.5344827586206896\n",
      "Epoch: 9\n",
      "tensor(0.5147, grad_fn=<DivBackward0>)\n",
      "0.8949889462048637\n",
      "0.5015673981191222\n",
      "Epoch: 10\n",
      "tensor(0.4989, grad_fn=<DivBackward0>)\n",
      "0.9045689019896831\n",
      "0.5015673981191222\n",
      "Epoch: 11\n",
      "tensor(0.4868, grad_fn=<DivBackward0>)\n",
      "0.9137803979366249\n",
      "0.5031347962382445\n",
      "Epoch: 12\n",
      "tensor(0.4800, grad_fn=<DivBackward0>)\n",
      "0.9266764922623434\n",
      "0.5\n",
      "Epoch: 13\n",
      "tensor(0.4715, grad_fn=<DivBackward0>)\n",
      "0.9325718496683861\n",
      "0.5062695924764891\n",
      "Epoch: 14\n",
      "tensor(0.4613, grad_fn=<DivBackward0>)\n",
      "0.9386514369933677\n",
      "0.5235109717868338\n",
      "Epoch: 15\n",
      "tensor(0.4520, grad_fn=<DivBackward0>)\n",
      "0.9463890935887989\n",
      "0.5203761755485894\n",
      "Epoch: 16\n",
      "tensor(0.4492, grad_fn=<DivBackward0>)\n",
      "0.9482313927781871\n",
      "0.5047021943573667\n",
      "Epoch: 17\n",
      "tensor(0.4403, grad_fn=<DivBackward0>)\n",
      "0.948415622697126\n",
      "0.49059561128526646\n",
      "Epoch: 18\n",
      "tensor(0.4372, grad_fn=<DivBackward0>)\n",
      "0.951731761238025\n",
      "0.5156739811912225\n",
      "Epoch: 19\n",
      "tensor(0.4343, grad_fn=<DivBackward0>)\n",
      "0.9579955784819455\n",
      "0.5078369905956113\n",
      "Epoch: 20\n",
      "tensor(0.4233, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Model Train \n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "ce = nn.CrossEntropyLoss()\n",
    "softmax = nn.Softmax(dim=0)\n",
    "optimizer = optim.SGD(our_wic.parameters(), lr=0.02)\n",
    "\n",
    "train_d = []\n",
    "val_d = []\n",
    "loss_d = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train_data)\n",
    "    print(\"Epoch:\",i)\n",
    "    total_loss = 0\n",
    "    for point in train_data:\n",
    "        our_wic.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # a) calculate probs / get an output\n",
    "        s1 = sen2vec(point[\"sentence1\"])\n",
    "        s2 = sen2vec(point[\"sentence2\"])\n",
    "        y_raw = our_wic(s1,s2)\n",
    "        #y_hat = softmax(y_raw)\n",
    "        \n",
    "        y = tensor(int(point[\"label\"]))\n",
    "        # b) compute loss\n",
    "        loss = ce(y_raw.unsqueeze(0),y.unsqueeze(0))\n",
    "        total_loss += loss\n",
    "        # c) get the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # d) update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(total_loss/len(train_data))\n",
    "    loss_d.append(total_loss/len(train_data))\n",
    "    \n",
    "    our_wic.eval()\n",
    "\n",
    "    score = 0\n",
    "    for point in train_data:\n",
    "        s1 = sen2vec(point['sentence1'])\n",
    "        s2 = sen2vec(point['sentence2'])\n",
    "        output = our_wic(s1,s2)\n",
    "        result = torch.argmax(softmax(output))\n",
    "        if bool(result) == point[\"label\"]:\n",
    "            score += 1\n",
    "\n",
    "    print(score/len(train_data))\n",
    "    train_d.append(score/len(train_data))\n",
    "    \n",
    "    score = 0\n",
    "    for point in val_data:\n",
    "        s1 = sen2vec(point['sentence1'])\n",
    "        s2 = sen2vec(point['sentence2'])\n",
    "        output = our_wic(s1,s2)\n",
    "        result = torch.argmax(softmax(output))\n",
    "        if bool(result) == point[\"label\"]:\n",
    "            score += 1\n",
    "\n",
    "    print(score/len(val_data))\n",
    "    val_d.append(score/len(val_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
