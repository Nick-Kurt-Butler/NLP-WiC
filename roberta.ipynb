{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b79c547",
   "metadata": {},
   "source": [
    "The system has two LSTM layers with 50 units, <br>\n",
    "one for each context side, which concatenates the <br>\n",
    "outputs and passes that to a feedforward layer <br>\n",
    "with 64 neurons, followed by a dropout layer at <br>\n",
    "rate 0.5, and a final one-neuron output layer of <br>\n",
    "sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17209607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8d4c376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.models.roberta import RobertaModel\n",
    "roberta = RobertaModel.from_pretrained('roberta.base', checkpoint_file='model.pt')\n",
    "roberta.eval();  # disable dropout (or leave in train mode to finetune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "699993d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import loads\n",
    "\n",
    "def load_data(file_name):\n",
    "    data = []\n",
    "    with open(file_name) as file:\n",
    "            for line in file.readlines():\n",
    "                data.append(loads(line))\n",
    "    return data\n",
    "                \n",
    "def data2input(point,pad=48):\n",
    "    word = point[\"word\"]\n",
    "    s1 = point[\"sentence1\"]\n",
    "    s2 = point[\"sentence2\"]\n",
    "    t1 = roberta.encode(word,s1)\n",
    "    t2 = roberta.encode(word,s2)\n",
    "    p1 = torch.zeros(pad-len(t1))\n",
    "    t1 = torch.cat((t1,p1))\n",
    "    p2 = torch.zeros(pad-len(t2))\n",
    "    t2 = torch.cat((t2,p2))\n",
    "    return t1,t2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d30b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim0, dim1, output_dim): # output = number tags\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(dim0, dim1, 1, bias=False)\n",
    "        #self.hidden_layer = nn.Linear(2*dim1,dim2)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.output_layer = nn.Linear(2*dim1, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, t1,t2):\n",
    "\n",
    "        _,(hidden_rep1,_) = self.lstm(t1.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "        _,(hidden_rep2,_) = self.lstm(t2.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "        hidden_rep = torch.cat((hidden_rep1.squeeze(0).squeeze(0),hidden_rep2.squeeze(0).squeeze(0)))\n",
    "        \n",
    "        #final_hidden_rep = self.hidden_layer(hidden_rep)\n",
    "        \n",
    "        drop = self.dropout(hidden_rep)\n",
    "        \n",
    "        output = self.output_layer(drop.squeeze(0))\n",
    "        #output = self.relu(output)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec31a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data('train.jsonl')\n",
    "test_data = load_data('test.jsonl')\n",
    "val_data = load_data('val.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b8ec727",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_wic = NeuralNet(48,127,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b30b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "tensor(0.6940, grad_fn=<DivBackward0>)\n",
      "0.5442151805453206\n",
      "0.4952978056426332\n",
      "Epoch: 1\n",
      "tensor(0.6887, grad_fn=<DivBackward0>)\n",
      "0.5514001473839352\n",
      "0.49216300940438873\n",
      "Epoch: 2\n",
      "tensor(0.6880, grad_fn=<DivBackward0>)\n",
      "0.5484524686809138\n",
      "0.48589341692789967\n",
      "Epoch: 3\n",
      "tensor(0.6877, grad_fn=<DivBackward0>)\n",
      "0.5442151805453206\n",
      "0.5047021943573667\n",
      "Epoch: 4\n",
      "tensor(0.6862, grad_fn=<DivBackward0>)\n",
      "0.5473470891672808\n",
      "0.5109717868338558\n",
      "Epoch: 5\n",
      "tensor(0.6860, grad_fn=<DivBackward0>)\n",
      "0.5490051584377302\n",
      "0.5062695924764891\n",
      "Epoch: 6\n",
      "tensor(0.6840, grad_fn=<DivBackward0>)\n",
      "0.5342667649226235\n",
      "0.49843260188087773\n",
      "Epoch: 7\n",
      "tensor(0.6871, grad_fn=<DivBackward0>)\n",
      "0.5464259395725866\n",
      "0.512539184952978\n",
      "Epoch: 8\n",
      "tensor(0.6867, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Model Train \n",
    "\n",
    "epochs = 1000\n",
    "ce = nn.CrossEntropyLoss()\n",
    "softmax = nn.Softmax(dim=0)\n",
    "optimizer = optim.SGD(our_wic.parameters(), lr=0.02)\n",
    "\n",
    "optimizer = optim.SGD([\n",
    "                {'params': our_wic.parameters()},\n",
    "                {'params': roberta.parameters(), 'lr': 0.02}\n",
    "            ], lr=0.02)\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train_data)\n",
    "    print(\"Epoch:\",i)\n",
    "    total_loss = 0\n",
    "    for point in train_data:\n",
    "        our_wic.train()\n",
    "        roberta.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # a) calculate probs / get an output\n",
    "        t1,t2 = data2input(point)\n",
    "        y_raw = our_wic(t1,t2)\n",
    "        #y_hat = softmax(y_raw)\n",
    "        \n",
    "        y = tensor(int(point[\"label\"]))\n",
    "        # b) compute loss\n",
    "        loss = ce(y_raw.unsqueeze(0),y.unsqueeze(0))\n",
    "        total_loss += loss\n",
    "        # c) get the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # d) update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(total_loss/len(train_data))\n",
    "    \n",
    "    our_wic.eval()\n",
    "    roberta.eval()\n",
    "\n",
    "    score = 0\n",
    "    for point in train_data:\n",
    "        t1,t2 = data2input(point)\n",
    "        output = our_wic(t1,t2).argmax()\n",
    "        if bool(output) == point[\"label\"]:\n",
    "            score += 1\n",
    "\n",
    "    print(score/len(train_data))\n",
    "    \n",
    "    score = 0\n",
    "    for point in val_data:\n",
    "        t1,t2 = data2input(point)\n",
    "        output = our_wic(t1,t2).argmax()\n",
    "        if bool(output) == point[\"label\"]:\n",
    "            score += 1\n",
    "\n",
    "    print(score/len(val_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
