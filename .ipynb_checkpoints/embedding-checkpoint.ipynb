{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b79c547",
   "metadata": {},
   "source": [
    "The system has two LSTM layers with 50 units, <br>\n",
    "one for each context side, which concatenates the <br>\n",
    "outputs and passes that to a feedforward layer <br>\n",
    "with 64 neurons, followed by a dropout layer at <br>\n",
    "rate 0.5, and a final one-neuron output layer of <br>\n",
    "sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17209607",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the things\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import random\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "699993d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data and vocab\n",
    "def load_data(file_name):\n",
    "    data = []\n",
    "    vocab = {\"<UNK>\":0}\n",
    "    with open(file_name,'r') as file:\n",
    "            for line in file.readlines():\n",
    "                    line = json.loads(line)\n",
    "                    #create vocabulary from all unique words in all sentences\n",
    "                    line[\"sentence1\"] = line[\"sentence1\"].strip('.').strip(',').strip(\"?\").strip(\"!\").strip(\";\").strip(\":\")\n",
    "                    line[\"sentence2\"] = line[\"sentence2\"].strip('.').strip(',').strip(\"?\").strip(\"!\").strip(\";\").strip(\":\")\n",
    "                    sentence = line['sentence1'] + \" \" + line['sentence2']\n",
    "                    #strip all punctuation from vocab words\n",
    "                    words = sentence.split()\n",
    "                    #add if not already in vocab\n",
    "                    for word in words:\n",
    "                        if word not in vocab:\n",
    "                            #add word to vocab dict\n",
    "                            vocab[word] = len(vocab)\n",
    "                    #add line to data\n",
    "                    data.append(line)\n",
    "    return vocab, data\n",
    "\n",
    "def sen2vec(s):\n",
    "    v = []\n",
    "    for word in s.split():\n",
    "        try:\n",
    "            v.append(vocab[word])\n",
    "        except:\n",
    "            v.append(0)\n",
    "    return tensor(v).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d30b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim): # output = number tags\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, 1, bias=False)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.output_layer = nn.Linear(2*hidden_dim, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, s1, s2):\n",
    "\n",
    "        embed1 = torch.sum(self.embedding(s1),dim=1)\n",
    "        _,(hidden_rep1,_) = self.lstm(embed1.unsqueeze(0))\n",
    "\n",
    "        embed2 = torch.sum(self.embedding(s2),dim=1)\n",
    "        _,(hidden_rep2,_) = self.lstm(embed2.unsqueeze(0))\n",
    "\n",
    "        # Option 1 Concat\n",
    "        hidden_rep1 = hidden_rep1.squeeze(0).squeeze(0)\n",
    "        hidden_rep2 = hidden_rep2.squeeze(0).squeeze(0)\n",
    "\n",
    "        final_hidden_rep = torch.cat((hidden_rep1, hidden_rep2))\n",
    "\n",
    "        drop = self.dropout(final_hidden_rep)\n",
    "\n",
    "        output = self.sigmoid(self.output_layer(drop.squeeze(0)))\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec31a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, train_data = load_data('train.jsonl')\n",
    "_, test_data = load_data('test.jsonl')\n",
    "_, val_data = load_data('val.jsonl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b8ec727",
   "metadata": {},
   "outputs": [],
   "source": [
    "our_wic = NeuralNet(len(vocab),73,127,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b30b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "tensor(0.6877, grad_fn=<DivBackward0>)\n",
      "0.6099852616064849\n",
      "0.5658307210031348\n",
      "Epoch: 1\n",
      "tensor(0.6691, grad_fn=<DivBackward0>)\n",
      "0.6637803979366249\n",
      "0.5282131661442007\n",
      "Epoch: 2\n",
      "tensor(0.6452, grad_fn=<DivBackward0>)\n",
      "0.7103905674281503\n",
      "0.5611285266457681\n",
      "Epoch: 3\n",
      "tensor(0.6231, grad_fn=<DivBackward0>)\n",
      "0.75792188651437\n",
      "0.5532915360501567\n",
      "Epoch: 4\n",
      "tensor(0.5952, grad_fn=<DivBackward0>)\n",
      "0.7932940309506263\n",
      "0.5470219435736677\n",
      "Epoch: 5\n",
      "tensor(0.5752, grad_fn=<DivBackward0>)\n",
      "0.8157700810611643\n",
      "0.542319749216301\n",
      "Epoch: 6\n",
      "tensor(0.5567, grad_fn=<DivBackward0>)\n",
      "0.8441414885777451\n",
      "0.5501567398119123\n",
      "Epoch: 7\n",
      "tensor(0.5363, grad_fn=<DivBackward0>)\n",
      "0.8592483419307295\n",
      "0.5454545454545454\n",
      "Epoch: 8\n",
      "tensor(0.5236, grad_fn=<DivBackward0>)\n",
      "0.8828297715549005\n",
      "0.5454545454545454\n",
      "Epoch: 9\n",
      "tensor(0.5094, grad_fn=<DivBackward0>)\n",
      "0.8959100957995578\n",
      "0.5532915360501567\n",
      "Epoch: 10\n",
      "tensor(0.4954, grad_fn=<DivBackward0>)\n",
      "0.9058585114222549\n",
      "0.542319749216301\n",
      "Epoch: 11\n",
      "tensor(0.4861, grad_fn=<DivBackward0>)\n",
      "0.9183861459100958\n",
      "0.5438871473354232\n",
      "Epoch: 12\n",
      "tensor(0.4759, grad_fn=<DivBackward0>)\n",
      "0.9235445836403832\n",
      "0.554858934169279\n",
      "Epoch: 13\n",
      "tensor(0.4622, grad_fn=<DivBackward0>)\n",
      "0.9342299189388357\n",
      "0.5329153605015674\n",
      "Epoch: 14\n",
      "tensor(0.4568, grad_fn=<DivBackward0>)\n",
      "0.9397568165070007\n",
      "0.5360501567398119\n",
      "Epoch: 15\n",
      "tensor(0.4516, grad_fn=<DivBackward0>)\n",
      "0.94620486366986\n",
      "0.5313479623824452\n",
      "Epoch: 16\n",
      "tensor(0.4383, grad_fn=<DivBackward0>)\n",
      "0.9493367722918202\n",
      "0.5235109717868338\n",
      "Epoch: 17\n",
      "tensor(0.4349, grad_fn=<DivBackward0>)\n",
      "0.9509948415622698\n",
      "0.512539184952978\n",
      "Epoch: 18\n",
      "tensor(0.4288, grad_fn=<DivBackward0>)\n",
      "0.9522844509948416\n",
      "0.5250783699059561\n",
      "Epoch: 19\n",
      "tensor(0.4292, grad_fn=<DivBackward0>)\n",
      "0.9554163596168017\n",
      "0.5391849529780565\n",
      "Epoch: 20\n"
     ]
    }
   ],
   "source": [
    "# Model Train \n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "ce = nn.CrossEntropyLoss()\n",
    "softmax = nn.Softmax(dim=0)\n",
    "optimizer = optim.SGD(our_wic.parameters(), lr=0.02)\n",
    "\n",
    "train_d = []\n",
    "val_d = []\n",
    "loss_d = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    random.shuffle(train_data)\n",
    "    print(\"Epoch:\",i)\n",
    "    total_loss = 0\n",
    "    for point in train_data:\n",
    "        our_wic.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # a) calculate probs / get an output\n",
    "        s1 = sen2vec(point[\"sentence1\"])\n",
    "        s2 = sen2vec(point[\"sentence2\"])\n",
    "        y_raw = our_wic(s1,s2)\n",
    "        #y_hat = softmax(y_raw)\n",
    "        \n",
    "        y = tensor(int(point[\"label\"]))\n",
    "        # b) compute loss\n",
    "        loss = ce(y_raw.unsqueeze(0),y.unsqueeze(0))\n",
    "        total_loss += loss\n",
    "        # c) get the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # d) update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(total_loss/len(train_data))\n",
    "    loss_d.append(total_loss/len(train_data))\n",
    "    \n",
    "    our_wic.eval()\n",
    "\n",
    "    score = 0\n",
    "    for point in train_data:\n",
    "        s1 = sen2vec(point['sentence1'])\n",
    "        s2 = sen2vec(point['sentence2'])\n",
    "        output = our_wic(s1,s2)\n",
    "        result = torch.argmax(softmax(output))\n",
    "        if bool(result) == point[\"label\"]:\n",
    "            score += 1\n",
    "\n",
    "    print(score/len(train_data))\n",
    "    train_d.append(score/len(train_data))\n",
    "    \n",
    "    score = 0\n",
    "    for point in val_data:\n",
    "        s1 = sen2vec(point['sentence1'])\n",
    "        s2 = sen2vec(point['sentence2'])\n",
    "        output = our_wic(s1,s2)\n",
    "        result = torch.argmax(softmax(output))\n",
    "        if bool(result) == point[\"label\"]:\n",
    "            score += 1\n",
    "\n",
    "    print(score/len(val_data))\n",
    "    val_d.append(score/len(val_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b617254",
   "metadata": {},
   "source": [
    "Epoch: 0\n",
    "tensor(0.6892, grad_fn=<DivBackward0>)\n",
    "0.6184598378776713\n",
    "0.5391849529780565\n",
    "Epoch: 1\n",
    "tensor(0.6700, grad_fn=<DivBackward0>)\n",
    "0.6696757553426677\n",
    "0.5391849529780565\n",
    "Epoch: 2\n",
    "tensor(0.6462, grad_fn=<DivBackward0>)\n",
    "0.7151805453205601\n",
    "0.54858934169279\n",
    "Epoch: 3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
