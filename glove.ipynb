{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import re\n",
    "import random\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871a30a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\"<UNK>\":0}\n",
    "\n",
    "#load the data and vocab\n",
    "def load_data(file_name):\n",
    "    data = []\n",
    "    with open(file_name,'r') as file:\n",
    "            for line in file.readlines():\n",
    "                    line = json.loads(line)\n",
    "                    #create vocabulary from all unique words in all sentences\n",
    "                    sentence = line['sentence1'] + \" \" + line['sentence2']\n",
    "                    words = sen2list(sentence)\n",
    "                    #add if not already in vocab\n",
    "                    for word in words:\n",
    "                        if word not in vocab:\n",
    "                            #add word to vocab dict\n",
    "                            vocab[word] = len(vocab)\n",
    "                    #add line to data\n",
    "                    data.append(line)\n",
    "    return data\n",
    "\n",
    "def sen2list(s):\n",
    "    s = s.replace(\"'s\",\"\")\n",
    "    return re.findall(r\"[\\w']+|[.,!?;]\",s.lower())\n",
    "\n",
    "def sen2vec(s):\n",
    "    v = []\n",
    "    for word in sen2list(s):\n",
    "        try:\n",
    "            v.append(vocab[word])\n",
    "        except:\n",
    "            v.append(vocab[\"<UNK>\"])\n",
    "    return tensor(v).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "\n",
    "def get_pretrained(vocab,filename,dim):\n",
    "\tM = np.zeros((len(vocab),dim))\n",
    "\tM[0] = 2*np.random.rand(dim)-1\n",
    "\twith open(filename) as file:\n",
    "\t\tfor line in file.readlines():\n",
    "\t\t\tline = line.split()\n",
    "\t\t\tword = line[0]\n",
    "\t\t\tvec = np.array(line[1:])\n",
    "\t\t\tif word in vocab:\n",
    "\t\t\t\tM[vocab[word]] = vec\n",
    "\treturn torch.from_numpy(M).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ffab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "\tdef __init__(self, vocab_size, embedding_dim, lstm_dim, hidden_dim, output_dim): # output = number tags\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t#self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\t\tself.embedding = nn.Embedding.from_pretrained(get_pretrained(vocab,\"glove\"+str(embedding_dim)+\".txt\",embedding_dim))\n",
    "\t\t#replace embedding with word embeddings -> getglove\n",
    "\n",
    "\t\tself.lstm = nn.LSTM(embedding_dim, lstm_dim, num_layers=2, bias=False) # two layers\n",
    "\t\t#self.lstm = nn.LSTM(embedding_dim, lstm_dim, num_layers=1, bias=False) # two layers\n",
    "\n",
    "\t\tself.hidden_layer = nn.Linear(2*lstm_dim,hidden_dim)\n",
    "\n",
    "\t\tself.dropout = nn.Dropout(p=0.3)\n",
    "\t\tself.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "\t\tself.relu = nn.ReLU()\n",
    "\t\tself.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "\tdef forward(self, s1, s2):\n",
    "\n",
    "\n",
    "\t\tembed1 = torch.sum(self.embedding(s1),dim=1)\n",
    "\t\t_,(lstm_output1,_) = self.lstm(embed1.unsqueeze(0))\n",
    "\n",
    "\t\tembed2 = torch.sum(self.embedding(s2),dim=1)\n",
    "\t\t_,(lstm_output2,_) = self.lstm(embed2.unsqueeze(0))\n",
    "\n",
    "\t\tlstm_output1 = lstm_output1.squeeze(1)\n",
    "\t\tlstm_output2 = lstm_output2.squeeze(1)\n",
    "\n",
    "\t\tcat_rep = torch.cat((lstm_output1, lstm_output2),1)\n",
    "\t\t#2x128\n",
    "\t\t#print(cat_rep.shape)\n",
    "\n",
    "\t\t#what are the bloody dimensions?\n",
    "\n",
    "\t\tcat_rep = torch.sum(cat_rep,dim=0)\n",
    "\n",
    "\t\thidden_rep = self.hidden_layer(cat_rep)\n",
    "\t\t#print(hidden_rep.shape)\n",
    "\n",
    "\t\tdrop = self.dropout(hidden_rep)\n",
    "\t\t#print(drop.shape)\n",
    "\n",
    "\t\toutput = self.output_layer(drop)\n",
    "\t\t#print(\"ouput\",output.shape)\n",
    "\n",
    "\t\treturn output.squeeze(0).squeeze(0)\n",
    "\n",
    "################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc87ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab, train_data = load_data('train.jsonl')\n",
    "#_, test_data = load_data('test.jsonl')\n",
    "#_, val_data = load_data('val.jsonl')\n",
    "\n",
    "train_data = load_data('train.jsonl')\n",
    "test_data = load_data('test.jsonl')\n",
    "val_data = load_data('val.jsonl')\n",
    "\n",
    "random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af197212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "\n",
    "our_wic = NeuralNet(len(vocab),300,50,64,1)\n",
    "#prune.ln_structured(our_wic, name=\"hidden_layer\", amount=0.5, n=2, dim=0)\n",
    "\n",
    "parameters_to_prune = (\n",
    "    (our_wic.hidden_layer, 'weight'),\n",
    "    (our_wic.output_layer, 'weight'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef4b26e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove300.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6906/1387302028.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;31m#################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0mour_wic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;31m#prune.ln_structured(our_wic, name=\"hidden_layer\", amount=0.5, n=2, dim=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6906/1387302028.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_size, embedding_dim, lstm_dim, hidden_dim, output_dim)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;31m#self.embedding = nn.Embedding(vocab_size, embedding_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"glove\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0;31m#replace embedding with word embeddings -> getglove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6906/1387302028.py\u001b[0m in \u001b[0;36mget_pretrained\u001b[0;34m(vocab, filename, dim)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove300.txt'"
     ]
    }
   ],
   "source": [
    "##################\n",
    "\n",
    "ce = nn.BCELoss()\n",
    "softmax = nn.Softmax(dim=0)\n",
    "optimizer = optim.SGD(our_wic.parameters(), lr=0.02)\n",
    "sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "\n",
    "\tour_wic.train()\n",
    "\n",
    "\t#print(\"Epoch:\",i)\n",
    "\ttotal_loss = 0\n",
    "\tfor point in train_data:\n",
    "\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t# a) calculate probs / get an output\n",
    "\t\ts1 = sen2vec(point[\"sentence1\"])\n",
    "\t\ts2 = sen2vec(point[\"sentence2\"])\n",
    "\t\ty_raw = our_wic(s1,s2)\n",
    "\t\t#y_hat = softmax(y_raw)\n",
    "\n",
    "\t\ty = tensor(float(point[\"label\"]))\n",
    "\t\t# b) compute loss\n",
    "\t\ty_raw = sig(y_raw)\n",
    "\t\tloss = ce(y_raw,y)\n",
    "\t\ttotal_loss += loss\n",
    "\n",
    "\t\t# c) get the gradient\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\t# d) update the weights\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\tprint(total_loss/len(train_data))\n",
    "\n",
    "\n",
    "\tprune.global_unstructured(\n",
    "\t    parameters_to_prune,\n",
    "\t    pruning_method=prune.L1Unstructured,\n",
    "\t    amount=0.2,\n",
    "\t)\n",
    "\n",
    "\n",
    "\tour_wic.eval()\n",
    "\n",
    "\tscore = 0\n",
    "\tfor point in train_data:\n",
    "\t\ts1 = sen2vec(point['sentence1'])\n",
    "\t\ts2 = sen2vec(point['sentence2'])\n",
    "\t\toutput = our_wic(s1,s2)\n",
    "\t\tresult = True if output >= 0 else False\n",
    "\t\tif bool(result) == point[\"label\"]:\n",
    "\t\t\tscore += 1\n",
    "\n",
    "\tprint(\"epoch:\",epoch,\" train data \",score/len(train_data))\n",
    "\n",
    "\tscore = 0\n",
    "\tfor point in val_data:\n",
    "\t\ts1 = sen2vec(point['sentence1'])\n",
    "\t\ts2 = sen2vec(point['sentence2'])\n",
    "\t\toutput = our_wic(s1,s2)\n",
    "\t\tresult = True if output >= 0 else False\n",
    "\t\tif bool(result) == point[\"label\"]:\n",
    "\t\t\tscore += 1\n",
    "\n",
    "\tprint(\"epoch:\",epoch,\" val data \",score/len(val_data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
